<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title></title>
</head>

<body>
<h2>&quot;Datasets that change&quot; email discussion on dods-tech (Dec 2005)</h2>
<p>John wrote:</p>
<p>At Unidata, our main use of OPeNDAP is remote access of datasets, using  the NetCDF API on the client. We want to open a local NetCDF file or a  remote OPeNDAP dataset, and have these be essentially equivilent. So  the following thoughts are from the perspective of NetCDF state  management.</p>
<p>  In this context, what should a client application do if it  discovers that the dataset has changed? Generally, the DDX information  has already escaped the opendap/netcdf layer. When the application  makes a new call into the opendap/netcdf layerlayer, which discovers  the change, what should happen? The opendap/netcdf layer could refetch  the DDX and examine the changes and try to figure out the best thing to  do. This can be a hard thing to do. Otherwise it must give an error  back to the application, which will have to re-fetch the metadata,  rebuild its GUI, notify the user, etc. Thats a lot of burden on the  application, and not the best user experience. So how can we minimize  when that has to happen? <br />
  <br />
  Suppose we implement a &quot;Last-Modified&quot; or a DDX checksum, or some  way to indicate that the dataset has changed. In my aggregation server  scenario, this would happen every 15 minutes. In fact, we have datasets  that change even more often. So this would be happening all the time.</p>
<p> 
  If I return to thinking about my specific case of the aggregation  server, I see that its not all that bad:    </p>
<ol>
  <li>First, most of the time  we are only extending the dataset. The possibility that the client  doesnt know about recent data is probably not a big, fatal deal. </li>
  <li>In  the case where new data arrives that needs to be inserted into the  middle of the dataset, the server could simply not do it for that  client, IF it could keep track of which clients were making the  request. If a new client made a request, the server could create an  updated version of the dataset.</li>
  <li>The case where the data is being deleted is harder. Perhaps  the server could prevent data from being deleted until a client is  done. Perhaps it passes back missing data where the deleted files are  accessed, etc. In my use case this only happens once a day instead of  constantly. </li>
</ol>
<p>  So when I look at the specifics of this, I see that I can mitigate  the problem completely if im willing to work hard enough on the server.  But it seems that I do need to keep track of users, i.e sessions. The  principle would be:  <em><strong>&quot;If the dataset can change, the server must set a  session cookie. If the client establishes a session by returning the  cookie, the server must keep the client's view of the dataset from  changing during the lifetime of the session&quot; </strong></em><br />
  <br />
  Adding Last-Modified or DDX checksum seems like also a good idea,  so that clients that are smart enough can deal with changed datasets,  eg discover recently arrived data, etc. Here the principle might be: <em><strong>&quot;If the dataset can change, the server must set the  Last-Modified/DDX checksum header, and update it whenever the dataset  changes. Its up to the client whether to refetch the DDX or not.&quot; </strong></em></p>
<p>  So, what are the consequences of this proposal?</p>
<ol>
  <li>Servers that have changing datasets need to set session cookies, and Last-Modified/DDX checksum headers. </li>
  <li>The standard client libraries must look for any Cookie headers,  and return them on subsequent requests. They may look for the  Last-Modified/DDX checksum header. </li>
  <li>Other clients like wget can ignore cookies, and they will work just fine, as long as they dont assume anything about state. </li>
  <li>Clients that assume state and dont set cookies or check  Last-Modified/DDX checksum are going to get errors, which is the  current situation.</li>
</ol>
<hr />
Joe Sirott wrote:<br />
            <br />
This would seem to be (at least to me) pretty difficult to manage in  practice. What happens if a client accesses the server with a cookie  that is, say, 2 days old? Will the server have to fix up the data to be  compatible with the 2 day old DDX? How exactly would that work? This  would be especially troublesome with deleted data -- the server can't  be prevented from deleting data until the client is done because it has  no way of knowing <em>when</em> the client is done. And what about  aggregations of other OPeNDAP servers? Will those servers also have to  support session cookies as well?<br />
<br />
At some point, the server has to give up and tell the client that the  DDX is stale and the client will have to handle that. Since the client  has to handle this anyway, why not avoid all the complexity and cookies  and just use the Last-Modified header?<br />
<br />
How does the C or Java netCDF library handle the case where process A  reads the header from a netCDF file, process B modifies the file (for  example, removes a variable) and then process A attempts to read the  removed variable? This seems (somewhat) similar to me and perhaps the  OpenDap netCDF library could use the same semantics.
<hr />
<br />
Dialogue with Joe Sirott: <br />
<blockquote type="cite"> This would seem to be (at least to me) pretty difficult to manage in   practice. What happens if a client accesses the server with a cookie   that is, say, 2 days old? Will the server have to fix up the data to be   compatible with the 2 day old DDX? </blockquote>
<br />
My thought is to not leave any persistent cookies, i.e. none that outlive the session. The server would time out the sessions after some reasonable amount of time - say no activity for 30 minutes. The server will have to decide how long it can maintain the DDX state, its really a &quot;best effort&quot; rather than a guarentee. <br />
<br />
<br />
<blockquote type="cite">How exactly would that work? This   would be especially troublesome with deleted data -- the server can't be   prevented from deleting data until the client is done because it has no   way of knowing <em>/when/</em> the client is done. </blockquote>
<br />
Yes, deleted data is the hardest to deal with. I think my initial  implementation would probably just break the session when data gets  deleted. This would only happen once a day. Eventually I might try to  do something better. Note that I still need a session to know whether I should return  an error or not. In a stateless situation, I would just happily return  incorrect data. <br />
<br />
When is the session done? In the netcdf world, the user makes an  explicit &quot;close&quot; call. Ideally we could communicate that back to the  server. Otherwise, we will rely on session timeout to say the client is  done. <br />
<br />
<blockquote type="cite">And what about aggregations of   other OPeNDAP servers? Will those servers also have to support session   cookies as well? <br />
</blockquote>
 Id say that a server should support sessions if their data can change. <br />
<blockquote type="cite">  At some point, the server has to give up and tell the client that the   DDX is stale and the client will have to handle that. Since the client   has to handle this anyway, why not avoid all the complexity and cookies   and just use the Last-Modified header? <br />
</blockquote>
<br />
Yes, the client will have to handle that situation. Most likely it will pass an error to the application. The question is does that have to happen all the time, or just occasionally? <br />
<blockquote type="cite"> <br />
  How does the C or Java netCDF library handle the case where process A   reads the header from a netCDF file, process B modifies the file (for   example, removes a variable) and then process A attempts to read the   removed variable? This seems (somewhat) similar to me and perhaps the   OpenDap netCDF library could use the same semantics. <br />
</blockquote>
<p><br />
  In principle you could write readers that could handle netcdf files  being simultaneously modified in arbitrary ways. In practice it doesnt  happen, and I would say it would be a mistake, that you would be adding  too much complexity, and you should use a database or something else  for that case. <br />
  <br />
  There is one case where this makes sense, which is when new records  are being appended along the unlimited dimension. In that case, the  reader can do a synch() call (in Java, I think C is similar) to  discover what the values of the unlimited dimension is. <br />
  <br />
  <br />
  Ok, let me think through again the possibility of just using the  Last-Modified header. Suppose that the client sees  that the  Last-Modified changed, and gets a new DDX. It checks for my common case  that the file has merely been appended to. This is a non-trivial check,  but doable. If thats the case, it just keep track of that in case the  client calls synch(). If the DDX has changed in an incompatible way, it  has to send an error to the application. In my scenario below, this  would only happen once a day (file deletion) and if files arrive out of  order. <br />
  So maybe this could work. The biggest fly in the ointment is that  its not sufficient to just test the DDX, you also have to check the  coordinate data in order to detect if files arrive out of order. You  might consider these two tests to be specific to my server, should they  become part of the client libraries? If not, then each application has  to implement them. Are there other ways that a file might commonly  change? <br />
  <br />
In a sense, the question is do you put the burden on the client or  the server to deal with changing datasets? You will probably be  influenced by whether you write clients or servers, and whether your  client is stateless or stateful (eg netCDF). </p>
<hr />
<p>Conversation with John, Peter Cornillon, James Gallagher:<br />
  <br />
  Peter Cornillon wrote: <br />
</p>
<blockquote type="cite">
  <p><br />
    Would it be possible to add &quot;Last modified&quot; and &quot;How modified&quot;  attributes via something like the old .das file. Of course clients  would have to know about the attributes and how to use them, but that  is really no different from knowing about other metadata associated  with the data set. So the client would get the &quot;Last modified&quot;  attribute and then when it makes a data request, would get the new  version and compare it with the old one. If things have changed it  would get the &quot;How modified&quot; attribute and determine how to proceed. A  data set that has been deleted could have a dummy URL associated with  it that would deliver a DDX with, say, a variable called dummy and the  &quot;How modified&quot; attribute would have the value &quot;Deleted&quot;. This dummy URL  could be kept around for a while. Another alternative would be to pass  this information in an error message. The bottom line is that the  client is going to have to address the issue that the data set has  changed in some fashion, so it's really a question of how much work is  going to be required of the client. <br />
  </p>
</blockquote>
<p><br />
  John wrote:</p>
<blockquote>
  <p>    Adding a &quot;How modified&quot; could really help the client more easily do  the right thing when things change. Putting &quot;Last modified&quot; and &quot;How  modified&quot; into an attribute vs using HTTP headers has its tradeoffs.  Does it have to fetch the DAS each time it makes a data request? Or  does that attribute get sent along with the data DDS? <br />
    <br />
    What should the values of &quot;How modified&quot; be? I would need something like: <br />
    1) Appended &lt;dimName&gt; <br />
    2) Reordered &lt;dimName&gt; <br />
    3) Deleted &lt;dimName?&gt;</p>
</blockquote>
<p>Peter wrote</p>
<blockquote>
  <p>I'm not sure how this will be handled in the new version of OPeNDAP.  The idea that I was promoting is really based on a convention rather  than something that is added to the protocol. It would be similar in  nature to COARDS. The convention could also be that two variables be  added to any data set that changes. One might even make it part of  COARDS. Then, the client would look for these variables. If they are  not present, it would assume, maybe incorrectly, that the data set is  not changing or at least not changing quickly. If they are present, the  client would download the two variables to determine how to proceed.<br />
      </p>
</blockquote>
<p>  James wrote:</p>
<blockquote>
  <p>The idea of How-Modified is interesting. One thing to keep in mind  is that Last-Modified is part of HTTP/1.1 and we have to respect it's  current meaning. How-Modified is our own deal; we can put it in the DDX  (which holds attributes as well as variables) or headers or whatever. <br />
    <br />
    Regarding the introduction of sessions to the protocol, I think  this can be handled so that it does not make the DAP a fundamentally  stateful protocol, which I think is important. <br />
    <br />
    Suppose we introduce the idea that a client MAY get a cookie from a  server, indicating that a session as been created by it's request. It  MAY include that cookie with future requests, indicating to a server  that it requests the server honor the previous session, making the  current response relative to the data source's state at the time the  cookie was issued (I know, I used the word  ;-) .  In this case the server MUST either honor the request OR return an  error. There's no requirement that a server support sessions and no  requirement that a client support them either. The DAP still has  fundamentally stateless behavior, but also has the capability to tell  certain servers, 'Hey, I was here before and this is what things looked  like then.' Most servers won't support this, and neither will most  clients (my guess) but some will and it looks like an important  capability. <br />
    <br />
    Ppersonally I have a strong aversion to the inclusion of state in  any transport/access protocol. It generally introduces more problems  than it solves. However, the addition of an optional (completely)  session is consistent with other technologies and has proven to be a  successful solution for those without making things (much) worse. Two  things that come to mid are HTTP 1.1 persistent connections and  caching. If I knew more about other protocols, I might have more  examples. My guess is that there's some cautionary evidence lurking in  CORBA... <br />
    <br />
    The checksum idea seemed like it might be useful in this context,  but it was proposed for dealing with data sources that are essentially  static but _might_  change over the span of several years. How can a person know about such  a change? In effect these are similar to the SHA1SUMs we provide with  the tar.gz source files. So we should not loose sight of the checksums  in all of this. They are still useful, just not so much for this  problem. <br />
  </p>
</blockquote>
<p>John wrote:</p>
<blockquote>
  <p>Yes, this would solve my problem in the Agg Server, thanks for  summarizing it concisely. You might add that if the session is  established, the server should make a best effort not to let the  dataset change during the duration of the session. <br />
    <br />
    I will go ahead and try to implement this in the Agg Server, and  report on any further problems encountered before we make a final  decision on it.  Im pretty sure it will be easy enough to return  cookies in the client libraries, so I will likely also modify the java  netcdf/opendap client library. That way I will have a complete test of  the idea. </p>
</blockquote>
<hr />
<p>John wrote:</p>
<p>Heres a summary of where I am on this project: <br />
  <br />
  If you remember, the problem is to keep the opendap dataset  metadata from changing in a way that gives erroneous results silently,  for the case that a client has a stateful view of the dataset, e.g. the  netcdf-opendap clients. <br />
  <br />
  In my server, I had already implemented caching of datasets, so  that repeated requests to the same dataset would be efficient. Normally  I would lock the dataset for the duration of the request, then  immediately release it. What I do now is to reserve the dataset object  for that particular client by not releasing it until the session  expires. I then know what metadata the client assumes, and can satisfy  that if possible, and give an error message if not. <br />
  <br />
  It was quite easy to modify Java DODS DConnect class to add support  for session cookies. Just a few lines of code, and it only happens if  the client enables it. <br />
  <br />
  In the normal session negotiation, the server offers a cookie when  it gets the first client request, and if the client returns the cookie,  the server establishes the session. In the Tomcat framework, every  request generates a session object, which times out if the cookie is  not returned. I was worried somewhat about the overhead of this. More  importantly, it meant that I couldnt immediately lock the dataset, but  had to wait to see if the client returned the cookie. For those 2  reasons, I decided to have the client send a header to indicate that   it would, indeed accept cookies. So when the client sends a request, it  adds the header: <br />
  <br />
   
  X-Accept-Session: true <br />
  <br />
  This allows me to immediately lock the dataset, and to not bother  creating a session if the header is absent. I put the dataset object  into the session object, and retrieve it every request. The session  automatically times out after 30 minutes. <br />
  <br />
  It seemed silly to have the dataset stay locked an extra 30 minutes  when most of the time the client library knows exactly when its done,  namely when the client calls close() on the dataset. So I wanted to  send a message when close() was called. I decided to just add a new  suffix &quot;.close&quot; to the dataset name on a GET call. When the server sees  this, it unlocks the dataset and terminates the session. <br />
  <br />
  So in summary: <br />
  <strong><br />
Java-DODS Client Library:</strong></p>
<ol>
  <li>Add <em>X-Accept-Session: true</em> header on each request. </li>
  <li>Look for any cookies on the response. Return them on subsequest requests. </li>
  <li>A new method close() was added. If its called, send a message to server with dataset name and       &quot;.close&quot; suffix&quot;. </li>
</ol>
<p><strong>  TDS Server :</strong></p>
<ol>
  <li> Look for <em>X-Accept-Session: true</em> header on each request.     </li>
  <li>If exists, establish a session for the client, return a session id cookie. </li>
  <li>Cache the dataset object for the duration of the session. </li>
  <li>Detect any changes to the dataset and give an error back to the client if needed. </li>
  <li>On a           &quot;close&quot; message, close the session and release the dataset. </li>
  <li>Timeout any session that hasnt been used for 30 minutes. <br />
  </li>
</ol>
<p><br />
  <br />
</p>
<blockquote type="cite"> </blockquote>
<p><br />
</p>
</body>
</html>
