<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <title></title>
  <link href="../../cdm.css" rel="stylesheet" type="text/css"/>
</head>

<body>
<h2>&quot;Datasets that change&quot; email discussion on dods-tech (Dec 2005)</h2>

<p>John wrote:</p>

<p>At Unidata, our main use of OPeNDAP is remote access of datasets, using the NetCDF API on the client. We want to open a local NetCDF file or a remote OPeNDAP
  dataset, and have these be essentially equivilent. So the following thoughts are from the perspective of NetCDF state management.</p>

<p> In this context, what should a client application do if it discovers that the dataset has changed? Generally, the DDX information has already escaped the
  opendap/netcdf layer. When the application makes a new call into the opendap/netcdf layerlayer, which discovers the change, what should happen? The
  opendap/netcdf layer could refetch the DDX and examine the changes and try to figure out the best thing to do. This can be a hard thing to do. Otherwise it
  must give an error back to the application, which will have to re-fetch the metadata, rebuild its GUI, notify the user, etc. Thats a lot of burden on the
  application, and not the best user experience. So how can we minimize when that has to happen? <br/>
  <br/>
  Suppose we implement a &quot;Last-Modified&quot; or a DDX checksum, or some way to indicate that the dataset has changed. In my aggregation server scenario,
  this would happen every 15 minutes. In fact, we have datasets that change even more often. So this would be happening all the time.</p>

<p>
  If I return to thinking about my specific case of the aggregation server, I see that its not all that bad:    </p>
<ol>
  <li>First, most of the time we are only extending the dataset. The possibility that the client doesnt know about recent data is probably not a big, fatal
    deal.
  </li>
  <li>In the case where new data arrives that needs to be inserted into the middle of the dataset, the server could simply not do it for that client, IF it
    could keep track of which clients were making the request. If a new client made a request, the server could create an updated version of the dataset.
  </li>
  <li>The case where the data is being deleted is harder. Perhaps the server could prevent data from being deleted until a client is done. Perhaps it passes
    back missing data where the deleted files are accessed, etc. In my use case this only happens once a day instead of constantly.
  </li>
</ol>
<p> So when I look at the specifics of this, I see that I can mitigate the problem completely if im willing to work hard enough on the server. But it seems that
  I do need to keep track of users, i.e sessions. The principle would be:  <em><strong>&quot;If the dataset can change, the server must set a session cookie. If
    the client establishes a session by returning the cookie, the server must keep the client's view of the dataset from changing during the lifetime of the
    session&quot; </strong></em><br/>
  <br/>
  Adding Last-Modified or DDX checksum seems like also a good idea, so that clients that are smart enough can deal with changed datasets, eg discover recently
  arrived data, etc. Here the principle might be: <em><strong>&quot;If the dataset can change, the server must set the Last-Modified/DDX checksum header, and
    update it whenever the dataset changes. Its up to the client whether to refetch the DDX or not.&quot; </strong></em></p>

<p> So, what are the consequences of this proposal?</p>
<ol>
  <li>Servers that have changing datasets need to set session cookies, and Last-Modified/DDX checksum headers.</li>
  <li>The standard client libraries must look for any Cookie headers, and return them on subsequent requests. They may look for the Last-Modified/DDX checksum
    header.
  </li>
  <li>Other clients like wget can ignore cookies, and they will work just fine, as long as they dont assume anything about state.</li>
  <li>Clients that assume state and dont set cookies or check Last-Modified/DDX checksum are going to get errors, which is the current situation.</li>
</ol>
<hr/>
Joe Sirott wrote:<br/>
<br/>
This would seem to be (at least to me) pretty difficult to manage in practice. What happens if a client accesses the server with a cookie that is, say, 2 days
old? Will the server have to fix up the data to be compatible with the 2 day old DDX? How exactly would that work? This would be especially troublesome with
deleted data -- the server can't be prevented from deleting data until the client is done because it has no way of knowing <em>when</em> the client is done. And
what about aggregations of other OPeNDAP servers? Will those servers also have to support session cookies as well?<br/>
<br/>
At some point, the server has to give up and tell the client that the DDX is stale and the client will have to handle that. Since the client has to handle this
anyway, why not avoid all the complexity and cookies and just use the Last-Modified header?<br/>
<br/>
How does the C or Java netCDF library handle the case where process A reads the header from a netCDF file, process B modifies the file (for example, removes a
variable) and then process A attempts to read the removed variable? This seems (somewhat) similar to me and perhaps the OpenDap netCDF library could use the
same semantics.
<hr/>
<br/>
Dialogue with Joe Sirott: <br/>
<blockquote > This would seem to be (at least to me) pretty difficult to manage in practice. What happens if a client accesses the server with a
  cookie that is, say, 2 days old? Will the server have to fix up the data to be compatible with the 2 day old DDX?
</blockquote>
<br/>
My thought is to not leave any persistent cookies, i.e. none that outlive the session. The server would time out the sessions after some reasonable amount of
time - say no activity for 30 minutes. The server will have to decide how long it can maintain the DDX state, its really a &quot;best effort&quot; rather than a
guarantee. <br/>
<br/>
<br/>
<blockquote >How exactly would that work? This would be especially troublesome with deleted data -- the server can't be prevented from deleting data
  until the client is done because it has no way of knowing <em>/when/</em> the client is done.
</blockquote>
<br/>
Yes, deleted data is the hardest to deal with. I think my initial implementation would probably just break the session when data gets deleted. This would only
happen once a day. Eventually I might try to do something better. Note that I still need a session to know whether I should return an error or not. In a
stateless situation, I would just happily return incorrect data. <br/>
<br/>
When is the session done? In the netcdf world, the user makes an explicit &quot;close&quot; call. Ideally we could communicate that back to the server.
Otherwise, we will rely on session timeout to say the client is done. <br/>
<br/>
<blockquote >And what about aggregations of other OPeNDAP servers? Will those servers also have to support session cookies as well? <br/>
</blockquote>
Id say that a server should support sessions if their data can change. <br/>
<blockquote > At some point, the server has to give up and tell the client that the DDX is stale and the client will have to handle that. Since the
  client has to handle this anyway, why not avoid all the complexity and cookies and just use the Last-Modified header? <br/>
</blockquote>
<br/>
Yes, the client will have to handle that situation. Most likely it will pass an error to the application. The question is does that have to happen all the time,
or just occasionally? <br/>
<blockquote ><br/>
  How does the C or Java netCDF library handle the case where process A reads the header from a netCDF file, process B modifies the file (for example, removes a
  variable) and then process A attempts to read the removed variable? This seems (somewhat) similar to me and perhaps the OpenDap netCDF library could use the
  same semantics. <br/>
</blockquote>
<p><br/>
  In principle you could write readers that could handle netcdf files being simultaneously modified in arbitrary ways. In practice it doesn't happen, and I would
  say it would be a mistake, that you would be adding too much complexity, and you should use a database or something else for that case. <br/>
  <br/>
  There is one case where this makes sense, which is when new records are being appended along the unlimited dimension. In that case, the reader can do a
  synch() call (in Java, I think C is similar) to discover what the values of the unlimited dimension is. <br/>
  <br/>
  <br/>
  Ok, let me think through again the possibility of just using the Last-Modified header. Suppose that the client sees  that the Last-Modified changed, and gets
  a new DDX. It checks for my common case that the file has merely been appended to. This is a non-trivial check, but doable. If thats the case, it just keep
  track of that in case the client calls synch(). If the DDX has changed in an incompatible way, it has to send an error to the application. In my scenario
  below, this would only happen once a day (file deletion) and if files arrive out of order. <br/>
  So maybe this could work. The biggest fly in the ointment is that its not sufficient to just test the DDX, you also have to check the coordinate data in order
  to detect if files arrive out of order. You might consider these two tests to be specific to my server, should they become part of the client libraries? If
  not, then each application has to implement them. Are there other ways that a file might commonly change? <br/>
  <br/>
  In a sense, the question is do you put the burden on the client or the server to deal with changing datasets? You will probably be influenced by whether you
  write clients or servers, and whether your client is stateless or stateful (eg netCDF). </p>
<hr/>
<p>Conversation with John, Peter Cornillon, James Gallagher:<br/>
  <br/>
  Peter Cornillon wrote: <br/>
</p>
<blockquote >
  <p><br/>
    Would it be possible to add &quot;Last modified&quot; and &quot;How modified&quot; attributes via something like the old .das file. Of course clients would
    have to know about the attributes and how to use them, but that is really no different from knowing about other metadata associated with the data set. So
    the client would get the &quot;Last modified&quot; attribute and then when it makes a data request, would get the new version and compare it with the old
    one. If things have changed it would get the &quot;How modified&quot; attribute and determine how to proceed. A data set that has been deleted could have a
    dummy URL associated with it that would deliver a DDX with, say, a variable called dummy and the &quot;How modified&quot; attribute would have the value
    &quot;Deleted&quot;. This dummy URL could be kept around for a while. Another alternative would be to pass this information in an error message. The bottom
    line is that the client is going to have to address the issue that the data set has changed in some fashion, so it's really a question of how much work is
    going to be required of the client. <br/>
  </p>
</blockquote>
<p><br/>
  John wrote:</p>
<blockquote>
  <p> Adding a &quot;How modified&quot; could really help the client more easily do the right thing when things change. Putting &quot;Last modified&quot; and
    &quot;How modified&quot; into an attribute vs using HTTP headers has its tradeoffs. Does it have to fetch the DAS each time it makes a data request? Or does
    that attribute get sent along with the data DDS? <br/>
    <br/>
    What should the values of &quot;How modified&quot; be? I would need something like: <br/>
    1) Appended &lt;dimName&gt; <br/>
    2) Reordered &lt;dimName&gt; <br/>
    3) Deleted &lt;dimName?&gt;</p>
</blockquote>
<p>Peter wrote</p>
<blockquote>
  <p>I'm not sure how this will be handled in the new version of OPeNDAP. The idea that I was promoting is really based on a convention rather than something
    that is added to the protocol. It would be similar in nature to COARDS. The convention could also be that two variables be added to any data set that
    changes. One might even make it part of COARDS. Then, the client would look for these variables. If they are not present, it would assume, maybe
    incorrectly, that the data set is not changing or at least not changing quickly. If they are present, the client would download the two variables to
    determine how to proceed.<br/>
  </p>
</blockquote>
<p> James wrote:</p>
<blockquote>
  <p>The idea of How-Modified is interesting. One thing to keep in mind is that Last-Modified is part of HTTP/1.1 and we have to respect it's current meaning.
    How-Modified is our own deal; we can put it in the DDX (which holds attributes as well as variables) or headers or whatever. <br/>
    <br/>
    Regarding the introduction of sessions to the protocol, I think this can be handled so that it does not make the DAP a fundamentally stateful protocol,
    which I think is important. <br/>
    <br/>
    Suppose we introduce the idea that a client MAY get a cookie from a server, indicating that a session as been created by it's request. It MAY include that
    cookie with future requests, indicating to a server that it requests the server honor the previous session, making the current response relative to the data
    source's state at the time the cookie was issued (I know, I used the word ;-) . In this case the server MUST either honor the request OR return an error.
    There's no requirement that a server support sessions and no requirement that a client support them either. The DAP still has fundamentally stateless
    behavior, but also has the capability to tell certain servers, 'Hey, I was here before and this is what things looked like then.' Most servers won't support
    this, and neither will most clients (my guess) but some will and it looks like an important capability. <br/>
    <br/>
    Ppersonally I have a strong aversion to the inclusion of state in any transport/access protocol. It generally introduces more problems than it solves.
    However, the addition of an optional (completely) session is consistent with other technologies and has proven to be a successful solution for those without
    making things (much) worse. Two things that come to mid are HTTP 1.1 persistent connections and caching. If I knew more about other protocols, I might have
    more examples. My guess is that there's some cautionary evidence lurking in CORBA... <br/>
    <br/>
    The checksum idea seemed like it might be useful in this context, but it was proposed for dealing with data sources that are essentially static but _might_
    change over the span of several years. How can a person know about such a change? In effect these are similar to the SHA1SUMs we provide with the tar.gz
    source files. So we should not loose sight of the checksums in all of this. They are still useful, just not so much for this problem. <br/>
  </p>
</blockquote>
<p>John wrote:</p>
<blockquote>
  <p>Yes, this would solve my problem in the Agg Server, thanks for summarizing it concisely. You might add that if the session is established, the server
    should make a best effort not to let the dataset change during the duration of the session. <br/>
    <br/>
    I will go ahead and try to implement this in the Agg Server, and report on any further problems encountered before we make a final decision on it.  Im
    pretty sure it will be easy enough to return cookies in the client libraries, so I will likely also modify the java netcdf/opendap client library. That way
    I will have a complete test of the idea. </p>
</blockquote>
<hr/>
<p>John wrote:</p>

<p>Heres a summary of where I am on this project: <br/>
  <br/>
  If you remember, the problem is to keep the opendap dataset metadata from changing in a way that gives erroneous results silently, for the case that a client
  has a stateful view of the dataset, e.g. the netcdf-opendap clients. <br/>
  <br/>
  In my server, I had already implemented caching of datasets, so that repeated requests to the same dataset would be efficient. Normally I would lock the
  dataset for the duration of the request, then immediately release it. What I do now is to reserve the dataset object for that particular client by not
  releasing it until the session expires. I then know what metadata the client assumes, and can satisfy that if possible, and give an error message if not.
  <br/>
  <br/>
  It was quite easy to modify Java DODS DConnect class to add support for session cookies. Just a few lines of code, and it only happens if the client enables
  it. <br/>
  <br/>
  In the normal session negotiation, the server offers a cookie when it gets the first client request, and if the client returns the cookie, the server
  establishes the session. In the Tomcat framework, every request generates a session object, which times out if the cookie is not returned. I was worried
  somewhat about the overhead of this. More importantly, it meant that I couldnt immediately lock the dataset, but had to wait to see if the client returned the
  cookie. For those 2 reasons, I decided to have the client send a header to indicate that  it would, indeed accept cookies. So when the client sends a request,
  it adds the header: <br/>
  <br/>
   
  X-Accept-Session: true <br/>
  <br/>
  This allows me to immediately lock the dataset, and to not bother creating a session if the header is absent. I put the dataset object into the session
  object, and retrieve it every request. The session automatically times out after 30 minutes. <br/>
  <br/>
  It seemed silly to have the dataset stay locked an extra 30 minutes when most of the time the client library knows exactly when its done, namely when the
  client calls close() on the dataset. So I wanted to send a message when close() was called. I decided to just add a new suffix &quot;.close&quot; to the
  dataset name on a GET call. When the server sees this, it unlocks the dataset and terminates the session. <br/>
  <br/>
  So in summary: <br/>
  <strong><br/>
    Java-DODS Client Library:</strong></p>
<ol>
  <li>Add <em>X-Accept-Session: true</em> header on each request.</li>
  <li>Look for any cookies on the response. Return them on subsequest requests.</li>
  <li>A new method close() was added. If its called, send a message to server with dataset name and &quot;.close&quot; suffix&quot;.</li>
</ol>
<p><strong> TDS Server :</strong></p>
<ol>
  <li> Look for <em>X-Accept-Session: true</em> header on each request.</li>
  <li>If exists, establish a session for the client, return a session id cookie.</li>
  <li>Cache the dataset object for the duration of the session.</li>
  <li>Detect any changes to the dataset and give an error back to the client if needed.</li>
  <li>On a &quot;close&quot; message, close the session and release the dataset.</li>
  <li>Timeout any session that hasnt been used for 30 minutes. <br/>
  </li>
</ol>
<p><br/>
  <br/>
</p>
<blockquote ></blockquote>
<p><br/>
</p>
<script src="https://docs.unidata.ucar.edu/js/ncj46_add_in.js" type="text/javascript"></script>
</body>
</html>
