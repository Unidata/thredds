:source-highlighter: coderay
[[threddsDocs]]


Untitled Document
=================

== Memory use in the TDS

=== FileCache

If found in cache, check to see if lastModifed, eject if so. On open,
lock, on close, unlock.

* *RandomAccessFile*
* **NetcdfFile**: uses RandomAccessFile.acquire().
* *GridNetcdf:* not used
* **RadialDataSweepAdapter**: not used
* **FeatureDatasetImpl**: not used
* **CoverageDatasetImpl**: not used
* **GribCollectionImmutable**: not used

=== FileCacheGuava

* **GribCdmIndex**: Partitions use this to open component *ncx3* files,
keep object in cache for reuse. Data files are always opened/closed as
needed, through the RAF cache. Top partition object is kept in memory.
No RAF is kept locked.

 

 

From
https://motherlode.ucar.edu:9443/thredds/admin/debug?Caches/showCaches
50 files are shown, (all GFS_Global_0p5deg).

From MAT, it appears each dataset is holding on to 5.345M. total = 267M

150 variables, each data variable has a GridVariable, size = 14K. 150 *
14 = 2.1 M (so about half)

each GridVariable has an array of GribGridRecord, 216 bytes apiece, 60 *
216 = 13K

so dominated by 216 * (# grib records) bytes.

=== catalogs

120M

=== Fmrc

10 Fmrc objects 55M

20 GridDataset (2D,best)

2143 VariableDS 48M

524 Vstate2D 44M avg = 88K. one for each var. Q: can these be shared ??

each VState2D has object array and int array. size = nruns * ntimes.

large one of 133K has Object[11041] == 88K + int[11041] = 44K = 133K

common case is tht 1 file = 1 runtime. should be able to optimise that.
